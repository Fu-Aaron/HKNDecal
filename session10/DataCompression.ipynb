{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 198-071 Lab: Video Compression\n",
    "\n",
    "Contributors: Shivin Devgon, Henry Muller, Dominic Carrano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The line `from IPython.display import Video` may fail. If so, comment it out and don't run the `Video([filepath])` lines; you can just view the video files outside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import numpy as np\n",
    "import pywt\n",
    "import skvideo.io\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "It's often said that a picture is worth a thousand words. So, a moving picture, or video, is easily worth millions. Indeed, with instant access to a video recorder in our smartphones, we have the ability to generate a remarkable amount of content right at our fingertips. But just how much information is in, say, a 30-second video? If we assume a 1920x1080 pixel frame size, and 30 frames per second, then we can do a quick calculation to find the file size:\n",
    "\n",
    "$$30 \\text{ seconds} \\cdot \\dfrac{30 \\text{ frames}}{1 \\text{ second}}\\cdot \\dfrac{1920\\times 1080 \\text{ pixels }}{1 \\text{ frame}} \\cdot \\dfrac{3 \\text{ RGB channels}}{1 \\text{ pixel}} \\cdot \\dfrac{8 \\text{ bits}}{1 \\text{ RGB channel}} = 44789760000 \\text{ bits}$$\n",
    "\n",
    "Since there are $10^9$ bits in a gigabyte, this comes out to a $5.6$ GB file. Ouch! Even if we had the latest and greatest iPhone with a 256 GB hard drive, we could only take 45 thirty second videos before our phone ran out of memory! Clearly, we need a *much* more efficient way to store them. But how?\n",
    "\n",
    "## Lossy vs Lossless Compression\n",
    "\n",
    "What we really need is a good *compression* algorithm: some magic genie that takes a video file and gives back a new video file that still has the same visual information, but uses less space.\n",
    "\n",
    "<img src=\"comp_algo.png\" width=600px></img>\n",
    "\n",
    "Fundamentally, there are two types of compression: **lossy** and **lossless**. Since lossless compression is a very hard problem to solve - unless there is some inherent redundancy in your data, there's no way to throw away stuff and retain all the information - we'll only consider lossy compression methods in this assignment. Before we get into the details of how to design this genie, let's take a look at our video - run the cell below and watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"oski_tree.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's actually read in the video as a numpy array and see what its dimensions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oski = skvideo.io.vread(\"oski_tree.mp4\")\n",
    "f,h,w,c = oski.shape\n",
    "print(oski.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 4D array that is $150 \\times 240 \\times 426 \\times 3$. What do these numbers mean, though? They are:\n",
    "1. The number of frames. With a 5 second video and 30 frames per second, we get 150 frames.\n",
    "2. The number of y-axis pixels. This means each frame is a 426x240 image.\n",
    "3. The number of x-axis pixels.\n",
    "4. The number of color channels per pixel. Typically, this is 3, with each pixel made up of Red (R), Green (G), and Blue (B) components.\n",
    "\n",
    "Let's extract the R,G, and B components as separate videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = oski[:,:,:,0]\n",
    "G = oski[:,:,:,1]\n",
    "B = oski[:,:,:,2]\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the \"3\" dimension is not present in `R` since it is just a single component.\n",
    "\n",
    "Finally, let's flatten each of the 3 color channels into two-dimensional signals using `np.reshape`. Now, each frame will be represented by a 1D signal instead of a 2D image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = R.reshape((f,h*w))\n",
    "G = G.reshape((f,h*w))\n",
    "B = B.reshape((f,h*w))\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lossless Compression: Intelligently Throwing Data Away\n",
    "\n",
    "At a high level, lossless compression should make sense: we need to reduce our file size, and one way to do so is just throwing away data. However, we still want the end result to be a faithful representation of the original video, so we have to be careful about *how* we decide what to throw out. To take this idea to the extreme, we could reduce the file size to nothing by completely discarding the entire video, but then we would have nothing left to watch! \n",
    "\n",
    "Instead, we could remove, say, every other frame of the video. But this still takes out too much data and might be annoying to someone watching - since we've effectively cut the number of frames per second in half, we've cut down on the resolution quite a bit. In some scenarios, this idea of removing every other frame, called *downsampling*, works okay, but we want something more sophisticated. \n",
    "\n",
    "## Part 1: The Discrete Fourier Transform (DFT) for Compression\n",
    "\n",
    "One of the most important mathematical insights in history, first made by Joseph Fourier in 1822, is that any function can be represented as a weighted combination of sine and cosine waves at different frequencies. The Discrete Fourier Transform (DFT), ubiquitous in science and engineering, is the black box that takes in a signal (i.e. function) and gives us back these \"weights\" for how strong each sine or cosine wave that makes up the signal is. \n",
    "\n",
    "For example, in 1D, suppose we had an audio clip that could be represented as:\n",
    "\n",
    "$$f(x) = \\sin(2\\pi \\cdot 10 x) + .35 \\cos(2\\pi \\cdot 70 x) + .0001 \\cos(2\\pi \\cdot 1000 x)$$\n",
    "\n",
    "Then we see that this is a sum of a 10 Hz sine wave with amplitude 1, a 70 Hz cosine with amplitude .35, and a 1000 Hz cosine with amplitude .0001. Clearly, the 1000 Hz cosine is much weaker than the other two, so we could create an approximation to $f(x)$, which we'll call $\\hat{f}(x)$, given by\n",
    "\n",
    "$$\\hat{f}(x) = \\sin(2\\pi \\cdot 10 x) + .35 \\cos(2\\pi \\cdot 70 x)$$\n",
    "\n",
    "from just throwing out that third term! Instead of using 3 coefficients in the sine/cosine basis ($1, .35, .0001$) we can get away with just two ($1, .35$)! If each coefficient is represented as a 64-bit floating point number, we just went from 192 bits to 128, a saving of 33%!\n",
    "\n",
    "**This is the core idea behind lossy image compression algorithms: Represent the video as a combination of \"basis\" signals, throw out the coefficients that are close to zero so we don't have to store them, and then store the video using only the largest coefficients.**\n",
    "\n",
    "Rather than pick a certain value that the coefficients must be above to not get thrown out, people typically look at how much information is contained in the top x percent of the coefficients. We can look at this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft2c(im):\n",
    "    return np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(im), norm=\"ortho\"))\n",
    "\n",
    "def ifft2c(ksp):\n",
    "    return np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(ksp), norm=\"ortho\"))\n",
    "\n",
    "def garotte_thresh(im, thresh):\n",
    "    return pywt.threshold(im, thresh, mode='garotte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call `fft2c`, which turns the video signal into its representation as a sum of sines and cosines, and then does a \"thresholding\", aka throws away coefficients below a certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_fft = fft2c(R)                           # Convert video to be in terms of sines and cosines\n",
    "R_fft_thresh = garotte_thresh(R_fft, 23.2) # Remove the small coefficients so we don't have to store\n",
    "R_comp = ifft2c(R_fft_thresh)              # Convert video back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at just the R component of the first frame and compare the uncompressed to compressed. The colors will look a bit odd since we're only looking at the R component, not the sum of the R, G, and B components as we did with the original video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_re = R_comp.reshape((f,h,w))\n",
    "R_orig = R.reshape((f,h,w))\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(R_orig.real[0])\n",
    "plt.title(\"Uncompressed\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(R_re.real[0])\n",
    "plt.title(\"Compressed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Now let's see how much information is contained by the coefficients we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_information(percent_info, num):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.plot(np.arange(num)[::10000]/num, percent_info[::10000])\n",
    "    plt.title(\"Percentage of Information held by Percentage of Components\")\n",
    "    plt.xlabel(\"Percentage of Components\")\n",
    "    plt.ylabel(\"Percentage of Information\")\n",
    "    \n",
    "def component_vs_info(im, quality = 0.6, display_thresh=True):\n",
    "    im_sort = np.sort(np.abs(im.flatten()))\n",
    "    s = np.sum(im_sort)\n",
    "    num = im.shape[0]*im.shape[1]\n",
    "    percent_info = np.cumsum(im_sort)/s\n",
    "    plot_information(percent_info, num)\n",
    "    if display_thresh:\n",
    "        thresh = np.argmin(np.abs(percent_info - (1 - quality)))\n",
    "        print(\"To retain\", quality * 100, \"percent quality, we need to remove all values below\", im_sort[thresh])\n",
    "        print(\"The top\",np.round(100*(1-thresh/num),2),\"percent of components contain\",quality*100,\"percent of the information\")\n",
    "        return im_sort[thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_vs_info(R_fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a threshold of 23.2, which corresponds to using only **12.92% of the coefficients**! That seems crazy, but this preserved 60% of the information! This is why transform-based compression methods are so effective: they find a *sparse* representation of signals, so that most of the information is contained in only a few numbers (the coefficients).\n",
    "\n",
    "Finally, let's actually compress our video using this method and see how the whole thing looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(im, quality = 0.6):\n",
    "    im_sort = np.sort(np.abs(im.flatten()))\n",
    "    s = np.sum(im_sort)\n",
    "    percent_info = np.cumsum(im_sort)/s\n",
    "    thresh = np.argmin(np.abs(percent_info - (1 - quality)))\n",
    "    return im_sort[thresh]\n",
    "    \n",
    "def reconstruct(im, quality = 0.6):\n",
    "    im_fft = fft2c(im)\n",
    "    thresh = get_threshold(im_fft, quality)\n",
    "    im_thresh = garotte_thresh(im_fft, thresh)\n",
    "    return ifft2c(im_thresh).real\n",
    "\n",
    "def compress_video(vid, quality = 0.6):\n",
    "    frames, h, w, channels = vid.shape\n",
    "    R = vid[:,:,:,0].reshape((frames,h*w))\n",
    "    G = vid[:,:,:,1].reshape((frames,h*w))\n",
    "    B = vid[:,:,:,2].reshape((frames,h*w))\n",
    "    R, G, B = reconstruct(R, quality), reconstruct(G, quality), reconstruct(B, quality)\n",
    "    R, G, B = np.reshape(R,(frames, h, w)), np.reshape(G,(frames, h, w)), np.reshape(B,(frames, h, w))\n",
    "    vid_compressed = np.stack((R,G,B), axis = -1)\n",
    "    skvideo.io.vwrite(\"oski_tree_compressed.mp4\", vid_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_video(oski)\n",
    "Video(\"oski_tree_compressed.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! So, representing things as sines and cosines seems to work alright. But can we do even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Wavelet-Based Compression\n",
    "\n",
    "In Part 1, we said that if we represent videos as a sum of sines and cosines, most of the information, or \"weight\" will be contained in only a few of the sines and cosines. Indeed, we saw that 60% of the information was contained in just 12-13% of the coefficients. It's not obvious that sines and cosines are the most efficient way to represent images or videos, however. \n",
    "\n",
    "For example, let's consider the checkerboard pattern below:\n",
    "\n",
    "<img src=\"checkerboard.png\" width=150px></img>\n",
    "\n",
    "Trying to represent this pattern in terms of sines and cosines would yield catastrophic results: due to the sharp transitions between white and black, it would in fact take an **infinite** number of sines and cosines at different frequencies added together to produce this pattern! More generally, sines and cosines suck at dealing with sharp, box-like things. So, instead we can use **Haar functions**, which are just rectangles, which are great at dealing with box-like objects since they are boxes!\n",
    "\n",
    "More generally, we could use any set of *wavelets*, which is just some set of orthonormal basis functions that we choose so that our data has a sparse representation in that basis. The Haar family is the most popular, however, so we'll use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    return skvideo.io.vread(video_path)\n",
    "\n",
    "def get_haar_wavelet_coefficient_set(frames):\n",
    "    def dwt2_haar_coefficients(frames, frame_no, color_no):\n",
    "        return pywt.dwt2(frames[frame_no, :, :, color_no], 'haar')\n",
    "\n",
    "    haar_coefficient_set = [[None, None, None] for _ in range(len(frames))]\n",
    "    for frame_no in range(len(frames)):\n",
    "        for color_no in (0, 1, 2):\n",
    "            haar_coefficient_set[frame_no][color_no] = dwt2_haar_coefficients(frames, frame_no, color_no)\n",
    "    return haar_coefficient_set\n",
    "\n",
    "def threshold_coefficients(haar_coefficient_set, approx_thresh=1000, horiz_thresh=1000, vert_thresh=1000, diag_thresh=1000):\n",
    "    res = copy.deepcopy(haar_coefficient_set)\n",
    "    for frame_no in range(len(res)):\n",
    "        for color_no in (0, 1, 2):\n",
    "            LL = pywt.threshold(res[frame_no][color_no][0],    approx_thresh, mode='soft', substitute=0)\n",
    "            LH = pywt.threshold(res[frame_no][color_no][1][0], horiz_thresh,  mode='soft', substitute=0)\n",
    "            HL = pywt.threshold(res[frame_no][color_no][1][1], vert_thresh,   mode='soft', substitute=0)\n",
    "            HH = pywt.threshold(res[frame_no][color_no][1][2], diag_thresh,   mode='soft', substitute=0)\n",
    "            res[frame_no][color_no] = (LL, (LH, HL, HH))\n",
    "    return res\n",
    "\n",
    "def get_reconstructed_frames(haar_coefficient_set, original_frames_shape):\n",
    "    res = np.empty(original_frames_shape, dtype=int)\n",
    "    for frame_no in range(len(haar_coefficient_set)):\n",
    "        for color_no in (0, 1, 2):\n",
    "            res[frame_no, :, :, color_no] = pywt.idwt2(haar_coefficient_set[frame_no][color_no], 'haar')\n",
    "    return res\n",
    "\n",
    "def make_video_file(frames, name):\n",
    "    skvideo.io.vwrite(name, frames, inputdict={'-r': '30',}, outputdict={'-r': '30'})\n",
    "    \n",
    "def plot_all_transforms(frames, frame_no, color_no):\n",
    "    titles = ['Approximation', ' Horizontal detail',\n",
    "          'Vertical detail', 'Diagonal detail']\n",
    "    coeffs = pywt.dwt2(frames[frame_no, :, :, color_no], 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    for i, a in enumerate([LL, LH, HL, HH]):\n",
    "        ax = fig.add_subplot(1, 4, i + 1)\n",
    "        ax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\n",
    "        ax.set_title(titles[i], fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_information(percent_info, num):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(np.arange(num)[::10000]/num, percent_info[::10000])\n",
    "    plt.title(\"Percentage of Information held by Percentage of Components\")\n",
    "    plt.xlabel(\"Percentage of Components\")\n",
    "    plt.ylabel(\"Percentage of Information\")\n",
    "\n",
    "def component_vs_info(im, quality=0.6, display_thresh=True):\n",
    "    im_sort = np.sort(np.abs(im.flatten()))\n",
    "    s = np.sum(im_sort)\n",
    "    num = im.shape[0]*im.shape[1]\n",
    "    percent_info = np.cumsum(im_sort)/s\n",
    "    plot_information(percent_info, num)\n",
    "    if display_thresh:\n",
    "        thresh = np.argmin(np.abs(percent_info - (1 - quality)))\n",
    "        print(\"To retain\", quality * 100, \"percent quality, we need to remove all values below\", im_sort[thresh])\n",
    "        print(\"The top\",np.round(100*(1-thresh/num),2),\"percent of components contain\",quality*100,\"percent of the information\")\n",
    "\n",
    "def display_frame_transform(frames, frame_no, color_no, detail='approx', quality=0.6):\n",
    "    titles = ['Approximation', ' Horizontal detail', 'Vertical detail', 'Diagonal detail']\n",
    "    LL, (LH, HL, HH) = pywt.dwt2(frames[frame_no, :, :, 0], 'haar')\n",
    "    fig = plt.figure(figsize=(48, 5))\n",
    "\n",
    "    if detail == 'approx':\n",
    "        coeffs = LL\n",
    "        title = 'Approximation'\n",
    "    elif detail == 'horiz':\n",
    "        coeffs = LH\n",
    "        title = 'Horizontal detail'\n",
    "    elif detail == 'vert':\n",
    "        coeffs = HL\n",
    "        title = 'Vertical detail'\n",
    "    elif detail == 'diag':\n",
    "        coeffs = HH\n",
    "        title = 'Diagonal detail'\n",
    "    else:\n",
    "        print('Detail should be approx, horiz, vert, or diag')\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.imshow(coeffs, interpolation='nearest', cmap=plt.cm.gray)\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    component_vs_info(coeffs, quality=quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice things about wavelets is that they pick out different kinds of information based on how they're scaled and stretched. For example, let's look at the different information picked out by the horizontal, vertical, and diagonal \"detailed coefficients\", as well as the \"approximation coefficient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_frames('oski_tree.mp4')\n",
    "plot_all_transforms(frames, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the vertical detail coefficients pick out the features that change across the y-axis, and similarly with the horizontal and diagonal detail coefficients. The approximation coefficients pick out the high-level trends of the image rather than the more detailed coefficients, and thus more closely resembles the actual image. Let's see how information-rich the wavelet coefficients are compared to the DFT coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame_transform(frames, 0, 0, detail='approx', quality=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame_transform(frames, 0, 0, detail='horiz', quality=0.75);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame_transform(frames, 0, 0, detail='vert', quality=0.75);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame_transform(frames, 0, 0, detail='diag', quality=0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the same. The actual numbers will depend on the specific video/image, but generally wavelets contain more information in less coefficients, making them a better basis for compression. Let's see the final video after doing wavelet compression with 90% information retained for the approximation coefficients, and 75% information retained for the other coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwcs = get_haar_wavelet_coefficient_set(frames)\n",
    "thresholded = threshold_coefficients(hwcs, approx_thresh=226, horiz_thresh=13.5, vert_thresh=3.5, diag_thresh=10)\n",
    "recon = get_reconstructed_frames(thresholded, np.shape(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_video_file(recon, 'recon.mp4')\n",
    "Video('recon.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for lossy wavelet compression, quality reduction manifests as a reduction in brightness. Try playing with the `approx_thresh` value to brighten the compressed video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] *Wavelet Shrinkage Denoising Using the Non-Negative Garotte*. Gao, Hong-Ye. 1997. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.8935&rep=rep1&type=pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
